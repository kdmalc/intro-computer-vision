{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kdmalc/intro-computer-vision/blob/main/HW4_2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Name: Kai Malcolm\n",
        "\n",
        "NetID: km82\n",
        "\n",
        "Collaborators: NA"
      ],
      "metadata": {
        "id": "LT3k_MzHAY-R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## General instructions\n",
        "Please copy this colab notebook into your own Drive to edit. This notebook will also serve as your final submission report - please ensure that code cells run correctly, and that all non-code (text/latex) blocks are rendered correctly before submissing the file. Feel free to add any additional cells (code or text) you need. Please follow good coding, markdown, and presentation etiquette.\n",
        "\n",
        "__Please do not use any AI tools for this assignment.__\n",
        "\n",
        "\n",
        "## Submission instructions\n",
        "\n",
        "- Before submitting, please `run-all` the code. This will re-render your entire jupyter file cell by cell to produce all the outputs.\n",
        "\n",
        "- You are required to download the colab notebook as a `.ipynb` file and submit it to canvas. Please name your `.ipynb` file as `netid.ipynb`\n",
        "\n",
        "- Modify the text cell on top to include your name and the names of any collaborators from this class you worked with on this assignment.\n",
        "\n",
        "- Download a pdf of the executed colab notebook. You can use print -> save as pdf. Please name your `.pdf` file as `netid.pdf`.\n",
        "\n",
        "- Any extra images used in the homework should also be uploaded to canvas.\n",
        "\n",
        "- For simplicity, you can also upload a `netid.zip` file to canvas containing all solution files."
      ],
      "metadata": {
        "id": "xSfLGHQ6AZ33"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EjDYul5zmfFf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "558e9473-b670-479e-e655-583654142a23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import imageio.v2 as imageio\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils import data\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms as T\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 1"
      ],
      "metadata": {
        "id": "nz-maMM7ptjn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem 1.1: Basics of Autograd"
      ],
      "metadata": {
        "id": "hiM-x3Sumt5k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Taylor approximation to sin(x).\n",
        "x: (Tensor[float]) input value(s)\n",
        "n: (int) number of terms in Taylor approximation\n",
        "\n",
        "Output:\n",
        "(Tensor[float]) Taylor approximation to sin(x)\n",
        "\"\"\"\n",
        "def sin_taylor(x, n=10):\n",
        "  # f(x) ~ \\sum f^n(a)/(n!) * (x-a)^n\n",
        "  # Let a = 0 (assuming we are evaluating at 0)\n",
        "  # f(x) is just sin(x)\n",
        "  # n is the number of terms to use in the approx\n",
        "  result = 0\n",
        "  for k in range(n):\n",
        "      sign = 1 if k % 4 != 3 else -1\n",
        "      if k%2==0:\n",
        "          deriv = np.sin(0)\n",
        "      else:\n",
        "          deriv = np.cos(0)\n",
        "      #print(f\"k: {k}; sign: {sign}, deriv: {deriv}\")\n",
        "      term = sign * deriv / math.factorial(k) * (x ** k)\n",
        "      result += term\n",
        "  return result"
      ],
      "metadata": {
        "id": "m6po0oOmmh8F"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sin_taylor(np.pi / 2, 10))  # Should be close to 1"
      ],
      "metadata": {
        "id": "VrRp8jnqNdAx",
        "outputId": "ce17f5b7-555e-4e9a-9d09-79209a74234b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0000035425842861\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1b\n",
        "\n",
        "x = torch.tensor(np.pi / 4, dtype=torch.float64, requires_grad=True)\n",
        "y = sin_taylor(x, 10)\n",
        "# Use autograd to compute dy/dx\n",
        "y.backward()\n",
        "\n",
        "# Compare to exact derivative: cos(Ï€/4)\n",
        "print(\"My taylor approx answer:\", y.item())\n",
        "print(\"Autograd answer:\", x.grad.item())\n",
        "print(\"Gruond truth:\", np.cos(math.pi/4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkLPYMg4J8_Y",
        "outputId": "1ff78eb2-d407-44ba-97a4-79f028cb1f88"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My taylor approx answer: 0.7071067829368671\n",
            "Autograd answer: 0.7071068056832943\n",
            "Gruond truth: 0.7071067811865476\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1c\n",
        "\n",
        "x_npy = np.random.uniform(-math.pi, math.pi, size=100)\n",
        "x = torch.tensor(x_npy, dtype=torch.float32).to('cuda')\n",
        "x.requires_grad_()\n",
        "y = sin_taylor(x, 10)\n",
        "z = y.sum().backward()\n",
        "\n",
        "print(\"x.grad (calc dy/dx):\\n\", x.grad)\n",
        "print()\n",
        "print(\"Error: x.grad (calc dy/dx) VS ground truth (cosx):\\n\", x.grad - torch.cos(x))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "WAD5kHu-J89Z",
        "outputId": "f09ea4bc-604c-4328-8846-c4faf7304b62"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-6882dc73d296>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mx_npy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_npy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msin_taylor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"LAZY\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1d\n",
        "\n",
        "# Using the same variables from 1c\n",
        "dzdx = x.grad.cpu().numpy()\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(x_npy, dzdx, label=\"Autograd dz/dx\")\n",
        "plt.plot(np.sort(x_npy), np.cos(np.sort(x_npy)), label=\"cos(x)\", color='black')\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"Gradient\")\n",
        "plt.title(\"Comparing autograd gradient calc to ground truth gradient\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lWfOlqwCJ862"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem 1.2: Image Denoising"
      ],
      "metadata": {
        "id": "yB4q34KznxXz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Returns the x and y gradient images for input image I.\n",
        "Input:\n",
        "I: (Tensor) Image of shape (H, W, 3)\n",
        "\n",
        "Output:\n",
        "(Ix, Iy): (Tensor) Gradient images each of shape (H, W, 3)\n",
        "\"\"\"\n",
        "def get_spatial_gradients(I):\n",
        "  I = I.permute(2, 0, 1).unsqueeze(0) # Change I's shape from (H, W, 3) to (1, 3, H, W)\n",
        "  kx = torch.zeros(I.shape[1], I.shape[1], 3, 3).to(I.device)\n",
        "  ky = torch.zeros(I.shape[1], I.shape[1], 3, 3).to(I.device)\n",
        "\n",
        "  for i in range(3):\n",
        "    kx[i, i, 1, 1] = -1\n",
        "    kx[i, i, 1, 2] = 1\n",
        "    ky[i, i, 1, 1] = -1\n",
        "    ky[i, i, 2, 1] = 1\n",
        "\n",
        "  Ix = F.conv2d(I, kx, padding=1)\n",
        "  Iy = F.conv2d(I, ky, padding=1)\n",
        "  return Ix[0,...].permute(1,2,0), Iy[0,...].permute(1,2,0)\n",
        "\n",
        "\"\"\"\n",
        "Denoising objective function.\n",
        "Input:\n",
        "I, J: (Tensor) Images of shape (H, W, 3)\n",
        "alpha: (float) Regularization hyperparameter\n",
        "\n",
        "Output:\n",
        "loss: (Tensor[float])\n",
        "\"\"\"\n",
        "def denoising_loss(I, J, alpha):\n",
        "  dxJ, dyJ = get_spatial_gradients(J)\n",
        "  return torch.norm(I-J, p=1) + alpha*( torch.norm(dxJ, p=1) + torch.norm(dyJ, p=1))"
      ],
      "metadata": {
        "id": "WXFdZlcVqGG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = imageio.imread('/content/drive/My Drive/ELEC546/HW4/parrot_noisy.png') / 255.0\n",
        "I = torch.tensor(img, dtype=torch.float32)\n",
        "I = I.to('cuda')\n",
        "\n",
        "lr = 0.001 # Learning rate\n",
        "alpha = 10.0 # alpha\n",
        "n_iter = 50 # Number of iterations\n",
        "\n",
        "# Initialize J to be a copy of I, with gradients\n",
        "J = I.clone().detach().requires_grad_(True)\n",
        "loss_log = [0]*n_iter\n",
        "for i in range(n_iter):\n",
        "  loss = denoising_loss(I, J, alpha)\n",
        "  loss_log[i] = loss.item()\n",
        "  loss.backward()\n",
        "  with torch.no_grad():\n",
        "    J -= lr * J.grad\n",
        "    J.grad.zero_()\n",
        "# Detach and move result to CPU for plotting\n",
        "J_img = J.detach().cpu().squeeze()\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "# Original image\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.imshow(img, cmap='gray')\n",
        "plt.title(\"Original Image\")\n",
        "plt.axis('off')\n",
        "# Smoothed image\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.imshow(np.clip(J_img.numpy(), 0.0, 1.0), cmap='gray')\n",
        "plt.title(\"Smoothed Image J\")\n",
        "plt.axis('off')\n",
        "# Loss curve\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.plot(loss_log)\n",
        "plt.title(\"Loss Curve\")\n",
        "plt.xlabel(\"Iter\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JD3Bz8Nbnz3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.2c\n",
        "\n",
        "def denoising_loss_L2(I, J, alpha):\n",
        "  dxJ, dyJ = get_spatial_gradients(J)\n",
        "  return torch.norm(I-J, p=2) + alpha*( torch.norm(dxJ, p=2) + torch.norm(dyJ, p=2))\n",
        "\n",
        "I = torch.tensor(img, dtype=torch.float32)\n",
        "I = I.to('cuda')\n",
        "\n",
        "lr = 0.001 # Learning rate\n",
        "alpha = 10.0 # alpha\n",
        "n_iter = 50 # Number of iterations\n",
        "\n",
        "# Initialize J to be a copy of I, with gradients\n",
        "J = I.clone().detach().requires_grad_(True)\n",
        "loss_log = [0]*n_iter\n",
        "for i in range(n_iter):\n",
        "  loss = denoising_loss_L2(I, J, alpha)\n",
        "  loss_log[i] = loss.item()\n",
        "  loss.backward()\n",
        "  with torch.no_grad():\n",
        "    J -= lr * J.grad\n",
        "    J.grad.zero_()\n",
        "# Detach and move result to CPU for plotting\n",
        "J_img = J.detach().cpu().squeeze()\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "# Original image\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.imshow(img, cmap='gray')\n",
        "plt.title(\"Original Image\")\n",
        "plt.axis('off')\n",
        "# Smoothed image\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.imshow(np.clip(J_img.numpy(), 0.0, 1.0), cmap='gray')\n",
        "plt.title(\"Smoothed Image J\")\n",
        "plt.axis('off')\n",
        "# Loss curve\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.plot(loss_log)\n",
        "plt.title(\"Loss Curve\")\n",
        "plt.xlabel(\"Iter\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6ot2n4ifTp3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "I = torch.tensor(img, dtype=torch.float32)\n",
        "I = I.to('cuda')\n",
        "\n",
        "lr = 0.01 # Learning rate\n",
        "alpha = 50.0 # alpha\n",
        "n_iter = 500 # Number of iterations\n",
        "\n",
        "# Initialize J to be a copy of I, with gradients\n",
        "J = I.clone().detach().requires_grad_(True)\n",
        "loss_log = [0]*n_iter\n",
        "for i in range(n_iter):\n",
        "  loss = denoising_loss_L2(I, J, alpha)\n",
        "  loss_log[i] = loss.item()\n",
        "  loss.backward()\n",
        "  with torch.no_grad():\n",
        "    J -= lr * J.grad\n",
        "    J.grad.zero_()\n",
        "# Detach and move result to CPU for plotting\n",
        "J_img = J.detach().cpu().squeeze()\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "# Original image\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.imshow(img, cmap='gray')\n",
        "plt.title(\"Original Image\")\n",
        "plt.axis('off')\n",
        "# Smoothed image\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.imshow(np.clip(J_img.numpy(), 0.0, 1.0), cmap='gray')\n",
        "plt.title(\"Smoothed Image J\")\n",
        "plt.axis('off')\n",
        "# Loss curve\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.plot(loss_log)\n",
        "plt.title(\"Loss Curve\")\n",
        "plt.xlabel(\"Iter\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Zl1jaEMDT_-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In my opinion, L2 is much worse. Basically, you can see that it did not work with the same hyperparameters as L1 (not that suprising), but the set of hyperparameters I found that worked 1) don't work that well and 2) are much slower to train (require many more iterations)."
      ],
      "metadata": {
        "id": "dKhZCMF0UaCu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 2: Training an Image Classifier"
      ],
      "metadata": {
        "id": "ZsCVB_DyDZfE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2a: Finish implementing dataset class"
      ],
      "metadata": {
        "id": "EFYCfKH1UD5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CIFARDataset(Dataset):\n",
        "  def __init__(self, images, labels, mode, transform):\n",
        "    self.transform = transform\n",
        "\n",
        "    if mode == 'train':\n",
        "      # Your code here. If training, use examples [0,40000) of the entire dataset\n",
        "      self.images = images[0:40000]\n",
        "      self.labels = labels[0:40000]\n",
        "\n",
        "    elif mode == 'val':\n",
        "      # Your code here. If validation, use examples [40,000-50,000) of the entire dataset\n",
        "      self.images = images[40000:50000]\n",
        "      self.labels = labels[40000:50000]\n",
        "\n",
        "    elif mode == 'test':\n",
        "      # Your code here. If testing, use examples [50,000-60,000) of the entire dataset\n",
        "      self.images = images[50000:60000]\n",
        "      self.labels = labels[50000:60000]\n",
        "\n",
        "    else:\n",
        "      raise ValueError('Invalid mode!')\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    # Do the following:\n",
        "    # 1. Get the image and label from the dataset corresponding to index idx.\n",
        "    image = self.images[idx]\n",
        "    label = self.labels[idx]\n",
        "    # 2. Convert the label to a LongTensor (needs to be of this type because it\n",
        "    # is an integer value and PyTorch will throw an error otherwise)\n",
        "    label = torch.tensor(label, dtype=torch.long)\n",
        "    # 3. Transform the image using self.transform. This will convert the image\n",
        "    # into a tensor, scale it to [0,1], and apply data augmentations.\n",
        "    if self.transform is not None:\n",
        "        image = self.transform(image)\n",
        "    # 4. Return the image and label.\n",
        "    return image, label\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images)"
      ],
      "metadata": {
        "id": "hMRFC93hpfVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2b: Write transforms"
      ],
      "metadata": {
        "id": "vb_AS9u6UKcv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms import Compose, ToTensor, RandomHorizontalFlip, RandomAffine, ColorJitter\n",
        "\n",
        "cifar = np.load('/content/drive/MyDrive/ELEC546/HW4/CIFAR.npz')\n",
        "X, y, label_names = cifar['X'], cifar['y'] * 1.0, cifar['label_names']\n",
        "print(\"Read in CIFAR10 dataset with %d examples, and labels:\\n %s\" % (X.shape[0], label_names))\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "transform = Compose([\n",
        "    ToTensor(),\n",
        "    RandomHorizontalFlip(),\n",
        "    RandomAffine(degrees=5, scale=(0.8, 1.2)),\n",
        "    ColorJitter(brightness=(0.8, 1.2), saturation=(0.8, 1.2))\n",
        "])\n",
        "train_dataset = CIFARDataset(X, y, \"train\", transform)\n",
        "train_dataloader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
        "\n",
        "transform = Compose([ToTensor()])\n",
        "val_dataset = CIFARDataset(X, y, \"val\", transform)\n",
        "val_dataloader = data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
        "\n",
        "transform = Compose([ToTensor()])\n",
        "test_dataset = CIFARDataset(X, y, \"test\", transform)\n",
        "test_dataloader = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=1)"
      ],
      "metadata": {
        "id": "sd6nAkFRzL9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2c: Implement the model"
      ],
      "metadata": {
        "id": "7UtuM7SfURSt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # Have to use padding otherwise kernel_size is too big\n",
        "    self.conv1 = nn.Conv2d(3, 50, kernel_size=3, padding=1)\n",
        "    self.conv2 = nn.Conv2d(50, 100, kernel_size=3, padding=1)\n",
        "    self.conv3 = nn.Conv2d(100, 100, kernel_size=3, padding=1)\n",
        "    self.conv4 = nn.Conv2d(100, 100, kernel_size=3, padding=1)\n",
        "    self.pool = nn.MaxPool2d(2, 2)\n",
        "    self.fc1 = nn.Linear(100 * 4 * 4, 100)  # This is hardcoded in, but that's fine I think since the model isn't dynamic (can only be resized manually)\n",
        "    self.fc2 = nn.Linear(100, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.conv1(x))\n",
        "    x = self.pool(x)\n",
        "\n",
        "    x = F.relu(self.conv2(x))\n",
        "    x = self.pool(x)\n",
        "\n",
        "    x = F.relu(self.conv3(x))\n",
        "    x = self.pool(x)\n",
        "\n",
        "    x = F.relu(self.conv4(x))\n",
        "\n",
        "    x = x.view(x.size(0), -1)  # Flatten to 2D, first dim is batchsize\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = self.fc2(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "0b11er66za2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2d/2e: Implement the training loop, and train your model."
      ],
      "metadata": {
        "id": "pXPzzAtZUU8L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here. Don't forget to call model.train() before training!\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "import os\n",
        "\n",
        "def train_model(model, train_dataset, val_dataset, save_dir, lr=1e-4, bs=64, num_epochs=15, device=\"cuda\"):\n",
        "    train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=bs, shuffle=False)\n",
        "\n",
        "    model = model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "\n",
        "    val_losses = []\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}, Training Loss: {total_loss:.4f}\")\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        val_losses.append(avg_val_loss)\n",
        "        print(f\"Epoch {epoch+1}, Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "        # Save checkpoint\n",
        "        torch.save(model.state_dict(), os.path.join(save_dir, f'2e_model_epoch_{epoch+1}.pth'))\n",
        "\n",
        "    return val_losses\n"
      ],
      "metadata": {
        "id": "n3h8ng4Bu4yZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2e\n",
        "\n",
        "model = Model()\n",
        "val_losses = train_model(model, train_dataset, val_dataset, save_dir='/content/drive/MyDrive/ELEC546/HW4/checkpoints')\n"
      ],
      "metadata": {
        "id": "Fmo1slSxiAaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2f: Choose the best model based on overall accuracy\n",
        "using the validation dataset."
      ],
      "metadata": {
        "id": "c09bhrEzUYZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# I just eval the model inside the training loop since that's how I normally do it...\n",
        "## So, just plotting the results here:\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(val_losses)\n",
        "plt.title(\"Loss Curve\")\n",
        "plt.xlabel(\"Iter\")\n",
        "plt.ylabel(\"Val Loss\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "strJmqVJGkBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I choose the last model because it has the lowest validation loss. The model appears to not have overfit at all. The validation loss has not plateaued yet"
      ],
      "metadata": {
        "id": "ciBjRTzyso9j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2g: Implement code for computing overall accuracy, accuracy per class, and the confusion matrix on the test set."
      ],
      "metadata": {
        "id": "1aNs9SvXUbv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here. Don't forget to call model.eval() first!\n",
        "\n",
        "device = \"cuda\"\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "# Load in model 15\n",
        "model = Model().to(device)\n",
        "model.load_state_dict(torch.load('/content/drive/My Drive/ELEC546/HW4/checkpoints/model_epoch_15.pth'))\n",
        "model.eval()\n",
        "\n",
        "# Calc overall accuracy\n",
        "val_loader = DataLoader(val_dataset, batch_size=bs, shuffle=False)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "model.eval()\n",
        "val_loss = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in val_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        val_loss += loss.item()\n",
        "avg_val_loss = val_loss / len(val_loader)\n",
        "val_losses.append(avg_val_loss)\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "with torch.no_grad():\n",
        "    for images, labels in val_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        preds = outputs.argmax(dim=1)\n",
        "\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "overall_accuracy = correct / total\n",
        "print(f\"Overall accuracy: {overall_accuracy:.4f}\")\n",
        "\n",
        "# Calc accuracy per class\n",
        "all_preds = np.array(all_preds)\n",
        "all_labels = np.array(all_labels)\n",
        "per_class_acc = []\n",
        "for i in range(len(class_names)):\n",
        "    correct_i = np.sum((all_preds == i) & (all_labels == i))\n",
        "    total_i = np.sum(all_labels == i)\n",
        "    acc_i = correct_i / total_i\n",
        "    per_class_acc.append(acc_i)\n",
        "    print(f\"{class_names[i]} accuracy: {acc_i:.4f}\")\n",
        "\n",
        "# Create the confusion matrix (10x10)\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "conf_mat = confusion_matrix(all_labels, all_preds, normalize='true')\n",
        "plt.figure()\n",
        "sns.heatmap(conf_mat, annot=True, fmt=\".2f\", cmap='Blues',\n",
        "            xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix (Normalized)\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IMO_cqS9GqEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For the class with the worst accuracy, what class is it most often confused with? Show 5-10 test images and comment on similarities\n",
        "\n",
        "# Find the class with worst accuracy\n",
        "worst_class_idx = np.argmin(per_class_acc)\n",
        "worst_class = class_names[worst_class_idx]\n",
        "# Find the most confused class\n",
        "confused_with_idx = np.argmax(conf_mat[worst_class_idx, :])\n",
        "confused_with_class = class_names[confused_with_idx]\n",
        "print(f\"Worst performing class: {worst_class}\")\n",
        "print(f\"Most confused with: {confused_with_class}\")\n"
      ],
      "metadata": {
        "id": "hmIYjAY8u2Os"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get indices of misclassified examples for this pair\n",
        "misclassified_indices = np.where(\n",
        "    (all_labels == worst_class_idx) & (all_preds == confused_with_idx)\n",
        ")[0]\n",
        "\n",
        "# Plot 5 of them\n",
        "num_to_show = min(len(misclassified_indices), 5)\n",
        "fig, axes = plt.subplots(1, num_to_show, figsize=(15, 2))\n",
        "for i in range(num_to_show):\n",
        "    idx = misclassified_indices[i]\n",
        "    img, _ = test_dataset[idx]\n",
        "    img = img.permute(1, 2, 0).numpy()\n",
        "    axes[i].imshow(img)\n",
        "    axes[i].set_title(f\"True: {worst_class}\\nPred: {confused_with_class}\")\n",
        "    axes[i].axis('off')\n",
        "plt.suptitle(f\"{worst_class} misclassified as {confused_with_class}\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "5ebWAWDxu6FF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "import numpy as np\n",
        "import os, json, cv2, random\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "import detectron2\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog"
      ],
      "metadata": {
        "id": "GGtT7ZOOtyfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 3: Run Mask R-CNN with Detectron2\n",
        "\n",
        "> 3.1: Conceptual: Mask R-CNN extends Faster R-CNN by adding a mask prediction branch. Explain the role of this branch and why it is necessary for instance segmentation.\n",
        "\n",
        "Faster R-CNN can be broken up into stage 1: the backbone network and RPN that are run once per image and stage 2: crop/alignment, predicting object class, predicting box offsets that are run once per region. This allows Faster R-CNN to do relatively quick object detection and bounding, namely identifying objects and then drawing a box to locate them. Mask R-CNN has the same architecture, but with the addition of the mask prediction branch, which is necessary since Mask R-CNN has the goal of doing instance segmentation, i.e., classifying evvery pixel belonging to an object of interest, as opposed to Faster R-CNN's object detection which wraps a box around each object and states what object is in that box. Thus, Mask R-CNN has a more difficult task, as it must segment each individual pixel (within the identified region/box), which is an extra step from Faster R-CNN. The mask prediction branch is responsible for masking the pixels within each box in order to segment the actual object out from the background in its identified box. The mask prediction branch creates a binary mask for every single class within each box / region of interest, and this is what ultimately identifies the exact position of each object. Without the mask prediction branch, Mask R-CNN would be the exact same as Faster R-CNN."
      ],
      "metadata": {
        "id": "oKYC_8FDSRoa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: This is a faster way to install detectron2 in Colab, but it does not include all functionalities (e.g. compiled operators).\n",
        "\n",
        "import sys, os, distutils.core\n",
        "!git clone 'https://github.com/facebookresearch/detectron2'\n",
        "dist = distutils.core.run_setup(\"./detectron2/setup.py\")\n",
        "!python -m pip install {' '.join([f\"'{x}'\" for x in dist.install_requires])}\n",
        "sys.path.insert(0, os.path.abspath('./detectron2'))"
      ],
      "metadata": {
        "id": "YZTzyrGkSgVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "import numpy as np\n",
        "import os, json, cv2, random\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "import detectron2\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog"
      ],
      "metadata": {
        "id": "Cmz8CsIXSpMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load image\n",
        "im = cv2.imread('/content/drive/My Drive/ELEC546/HW4/train.jpg')\n",
        "cv2_imshow(im)"
      ],
      "metadata": {
        "id": "iygLaAumSv4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained model\n",
        "# You can find the model cofig from the following url, or other config of your choice\n",
        "# https://github.com/facebookresearch/detectron2/tree/main/configs/COCO-Detection\n",
        "# https://github.com/facebookresearch/detectron2/tree/main/configs/COCO-InstanceSegmentation\n",
        "\n",
        "##### Your code #####\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"your choice of pretrained model\"))\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"Your choice of pretrained model\")\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST =   # set threshold for this model (0-1)"
      ],
      "metadata": {
        "id": "QhGC0scvS4pm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create predictor\n",
        "predictor = DefaultPredictor(cfg)\n",
        "# Run inference on the sample image\n",
        "outputs = predictor(im)"
      ],
      "metadata": {
        "id": "SkG8CN7AVvKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize results with \"Visualizer\"\n",
        "v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
        "out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "cv2_imshow(out.get_image()[:, :, ::-1])"
      ],
      "metadata": {
        "id": "xgCa4P-MV3UH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 4: Adversarial attacks for trained networks. Wite code below"
      ],
      "metadata": {
        "id": "tcW7S6YT4glf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fgsm_attack(model, image, eps):\n",
        "  # Your code here."
      ],
      "metadata": {
        "id": "q_mMeLE34j0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here for creating and displaying adversarial images."
      ],
      "metadata": {
        "id": "xQhk7ImN70s8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}