{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kdmalc/intro-computer-vision/blob/main/HW5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Name:\n",
        "\n",
        "NetID:\n",
        "\n",
        "Collaborators:"
      ],
      "metadata": {
        "id": "wC1mmtMovkwe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# General instructions\n",
        "Please copy this colab notebook into your own Drive to edit. This notebook will also serve as your final submission report - please ensure that code cells run correctly, and that all non-code (text/latex) blocks are rendered correctly before submissing the file. Feel free to add any additional cells (code or text) you need. Please follow good coding, markdown, and presentation etiquette.\n",
        "\n",
        "__Please do not use any AI tools for this assignment.__\n",
        "\n",
        "\n",
        "## Submission instructions\n",
        "\n",
        "- Before submitting, please `run-all` the code. This will re-render your entire jupyter file cell by cell to produce all the outputs.\n",
        "\n",
        "- You are required to download the colab notebook as a `.ipynb` file and submit it to canvas. Please name your `.ipynb` file as `netid.ipynb`\n",
        "\n",
        "- Modify the text cell on top to include your name and the names of any collaborators from this class you worked with on this assignment.\n",
        "\n",
        "- Download a pdf of the executed colab notebook. You can use print -> save as pdf. Please name your `.pdf` file as `netid.pdf`.\n",
        "\n",
        "- Any extra images used in the homework should also be uploaded to canvas.\n",
        "\n",
        "- For simplicity, you can also upload a `netid.zip` file to canvas containing all solution files."
      ],
      "metadata": {
        "id": "CJEkBjwDvmjt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 1: Segmentation\n"
      ],
      "metadata": {
        "id": "4joCKy9yn8Hp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms.functional as TF\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils import data\n",
        "from torchvision import transforms as T\n",
        "from torchvision import models\n",
        "\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)"
      ],
      "metadata": {
        "id": "La8zfwEkn5cD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1eYYJ26R1S9Ln_ExwHFBqd3rbln9qVdi4&export=download\n",
        "!unzip -qq cityscapes.zip"
      ],
      "metadata": {
        "id": "_ANItEw3iVwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CityScapesDataset(Dataset):\n",
        "  def __init__(self, images, labels, im_transform, mask_transform):\n",
        "    self.images = images\n",
        "    self.labels = labels\n",
        "    self.im_transform = im_transform\n",
        "    self.mask_transform = mask_transform\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    im = Image.open(self.images[idx])\n",
        "    mask = Image.open(self.labels[idx])\n",
        "    im = self.im_transform(im)[0:3, ...] # Transform image\n",
        "\n",
        "    # Add an extra first dimension to mask (needed for transforms), convert\n",
        "    # to LongTensor b/c values are integers, and apply transforms.\n",
        "    mask = np.asarray(mask)[None, ...]\n",
        "    mask = torch.LongTensor(mask)\n",
        "    mask = self.mask_transform(mask)\n",
        "\n",
        "    # Apply random horizontal flip to image and mask\n",
        "    if np.random.rand() > 0.5:\n",
        "      im = TF.hflip(im)\n",
        "      mask  = TF.hflip(mask)\n",
        "\n",
        "    return im, mask\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images)"
      ],
      "metadata": {
        "id": "SHbzyW7AiogO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "\n",
        "# Make image and mask transforms.\n",
        "im_transform = [T.ToTensor()]\n",
        "im_transform.append(T.Resize((256, 256), interpolation=T.InterpolationMode.BILINEAR))\n",
        "im_transform = T.Compose(im_transform)\n",
        "\n",
        "mask_transform = T.Resize((256, 256), interpolation=T.InterpolationMode.NEAREST)\n",
        "\n",
        "def get_dataloader(im_path):\n",
        "  images = sorted(glob.glob(im_path + '/*8bit.jpg'))\n",
        "  labels = sorted(glob.glob(im_path + '/*labelIds.png'))\n",
        "  dataset = CityScapesDataset(images, labels, im_transform, mask_transform)\n",
        "  return data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
        "\n",
        "# Create dataloaders\n",
        "train_dataloader = get_dataloader('./cityscapes/train')\n",
        "val_dataloader = get_dataloader('./cityscapes/val')"
      ],
      "metadata": {
        "id": "zDAFeoLMjKts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 1a: Implement Segmentation model"
      ],
      "metadata": {
        "id": "5TGcGBABtMtK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Segmenter(torch.nn.Module):\n",
        "    def __init__(self, n_classes, encoder):\n",
        "        super(Segmenter, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        #self.decoder = Your code for Problem 1a goes here\n",
        "\n",
        "    def forward(self, x):\n",
        "      return None # Your code for Problem 1a goes here"
      ],
      "metadata": {
        "id": "eliKj_SjicPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get features from VGG16 up through 3 downsampling (maxpool) operations.\n",
        "vgg = models.vgg16(pretrained=True);\n",
        "encoder = nn.Sequential(*(list(vgg.children())[:1])[0][0:17]);\n",
        "\n",
        "# Create model\n",
        "n_classes = 34\n",
        "model = Segmenter(n_classes, encoder);\n",
        "model.to('cuda');"
      ],
      "metadata": {
        "id": "PyPKm5ZrkUt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 1b: Train your segmentation model"
      ],
      "metadata": {
        "id": "6ZbCjfLntFvp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 1e-4\n",
        "loss = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "num_epochs = 7\n",
        "\n",
        "# Problem 1b: Your training loop code goes here"
      ],
      "metadata": {
        "id": "Txwp1i6TkcJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 1c: Evaluate your model"
      ],
      "metadata": {
        "id": "PEwOBs7tt5o6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem 1c: Your IoU evaluation code goes here"
      ],
      "metadata": {
        "id": "Rlzt1R8skeAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 1d: Visualize validation images"
      ],
      "metadata": {
        "id": "Uk0WO1a9uB2q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem 1d: Your image results code goes here"
      ],
      "metadata": {
        "id": "5d6MY5IHkv_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 1e:\tLook at the lines of code for resizing the images and masks to 256 x 256. We use bilinear interpolation when resizing the image, but nearest neighbor interpolation when resizing the mask. Why do we not use bilinear interpolation for the mask?\n",
        "\n",
        "!!!YOUR ANSWER HERE!!!"
      ],
      "metadata": {
        "id": "zFbe2nd2xU8K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 1e:\tLook at the lines of code for resizing the images and masks to 256 x 256. We use bilinear interpolation when resizing the image, but nearest neighbor interpolation when resizing the mask. Why do we not use bilinear interpolation for the mask?\n",
        "\n",
        "!!!YOUR ANSWER HERE!!!"
      ],
      "metadata": {
        "id": "XPkXQR5mxVDp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 1f. Look at the `__getitem__` function for the `CityScapesDataset` class and notice that we apply a horizontal flip augmentation to the image and mask usnig a random number generator. Why do we apply the flip in this way instea of simply adding a `T.RandomHorizontalFlip` to the sequence of transforms in `im_transform` and `mask_transform` (similar to what you did in HWK 4)?\n",
        "\n",
        "!!! YOUR ANSWER HERE !!!"
      ],
      "metadata": {
        "id": "W_kKZ64axoGc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 2: Vision Transformers"
      ],
      "metadata": {
        "id": "McjyGSFvsLva"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib\n",
        "\n",
        "import io\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "def load_image_from_url(url: str) -> Image:\n",
        "    with urllib.request.urlopen(url) as f:\n",
        "        return Image.open(f).convert(\"RGB\")\n",
        "\n",
        "\n",
        "EXAMPLE_IMAGE_URL = \"https://dl.fbaipublicfiles.com/dinov2/images/example.jpg\"\n",
        "example_image = load_image_from_url(EXAMPLE_IMAGE_URL)\n",
        "display(example_image)"
      ],
      "metadata": {
        "id": "uwDJlWMlsKtx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision.models.feature_extraction import create_feature_extractor\n",
        "import torchvision.transforms as transforms\n",
        "import timm\n",
        "\n",
        "# Load DinoV2 with registers model\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "dinov2_model = timm.create_model('vit_base_patch14_reg4_dinov2', pretrained=True)\n",
        "dinov2_model.eval()\n",
        "dinov2_model.to(DEVICE)\n",
        "\n",
        "# Create feature extractor to output q, k values from last layer\n",
        "# and final output tokens from vision transformer\n",
        "dinov2_feature_extractor = create_feature_extractor(\n",
        "    dinov2_model, return_nodes=['blocks.11.attn.q_norm',\n",
        "                                'blocks.11.attn.k_norm',\n",
        "                                'norm'],\n",
        ")\n",
        "\n",
        "# Useful variables for model\n",
        "IMAGE_CHANNELS, IMAGE_HEIGHT, IMAGE_WIDTH = dinov2_model.pretrained_cfg['input_size']\n",
        "PATCH_SIZE, _ = dinov2_model.patch_embed.patch_size\n",
        "DINOV2_IMAGE_MEAN = dinov2_model.pretrained_cfg['mean']\n",
        "DINOV2_IMAGE_STD = dinov2_model.pretrained_cfg['std']\n",
        "NUM_PREFIX_TOKENS = dinov2_model.num_prefix_tokens\n",
        "dinov2_transforms = transforms.Compose([\n",
        "    transforms.Resize((IMAGE_HEIGHT, IMAGE_WIDTH),\n",
        "                      interpolation=Image.Resampling.BICUBIC,\n",
        "                      antialias=True),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=DINOV2_IMAGE_MEAN, std=DINOV2_IMAGE_STD),\n",
        "    ])"
      ],
      "metadata": {
        "id": "XIQmk77lsKv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Easy function to grab q, k, and final output tokens from DinoV2-Reg model\n",
        "def get_features_from_dinov2(image: Image.Image):\n",
        "  image_pt = dinov2_transforms(image).unsqueeze(0)\n",
        "  with torch.no_grad():\n",
        "    out = dinov2_feature_extractor(image_pt.to(DEVICE))\n",
        "  q = out['blocks.11.attn.q_norm'].squeeze(0).cpu()\n",
        "  k = out['blocks.11.attn.k_norm'].squeeze(0).cpu()\n",
        "  out_tokens = out['norm'].squeeze(0).cpu()\n",
        "  image_feat_tokens = out_tokens[NUM_PREFIX_TOKENS:, :]\n",
        "\n",
        "  return q, k, image_feat_tokens\n",
        "\n",
        "q, k, image_feat_tokens = get_features_from_dinov2(example_image)\n",
        "print(\"Q:\", q.shape)\n",
        "print(\"K:\", k.shape)\n",
        "print(\"Image feature tokens:\", image_feat_tokens.shape)"
      ],
      "metadata": {
        "id": "0zaZiJ0AsmTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to visualize self-attention with class embedding\n",
        "def visualize_class_attention(\n",
        "    attn_matrix: torch.Tensor,  # attention weights (n_heads, n_tokens, n_tokens)\n",
        "    num_prefix_tokens=5,\n",
        "    image_height=518,\n",
        "    image_width=518,\n",
        "    patch_size=14,\n",
        "    ncols=3):\n",
        "\n",
        "  assert (attn_matrix.ndim == 3), \"Attention map should have shape (n_heads, n_tokens, n_tokens)\"\n",
        "  assert (attn_matrix.shape[1] == attn_matrix.shape[2]), \"Attention map should be square\"\n",
        "  n_heads, n_tokens, _ = attn_matrix.shape\n",
        "  nrows = n_heads // ncols\n",
        "\n",
        "  fig, axs = plt.subplots(nrows, ncols, figsize=(10, 10))\n",
        "  for i, ax in enumerate(axs.flatten()):\n",
        "    # Get attention weights between class token and image tokens\n",
        "    class_token_attn = attn_matrix[i, 0, num_prefix_tokens:]\n",
        "    class_token_attn = class_token_attn.reshape(image_height // patch_size, image_width // patch_size)\n",
        "    # Plotting\n",
        "    ax.imshow(class_token_attn, cmap='hot', aspect='auto')\n",
        "    ax.axis('off')\n",
        "  plt.subplots_adjust(hspace=0.1, wspace=0.1)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "0i63n2YdsmVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 2a: Visualize self-attention of vision transformer"
      ],
      "metadata": {
        "id": "fg8gMSgouotc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_attention_weight(q, k):\n",
        "  # Your code for Problem 2a goes here\n",
        "  pass"
      ],
      "metadata": {
        "id": "sHa5DZT2smW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_class_attention(compute_attention_weight(q, k),\n",
        "                          num_prefix_tokens=NUM_PREFIX_TOKENS,\n",
        "                          image_height=IMAGE_HEIGHT,\n",
        "                          image_width=IMAGE_WIDTH,\n",
        "                          patch_size=PATCH_SIZE)"
      ],
      "metadata": {
        "id": "WU-BPdnFsmb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 2a: Comment on the similarities and differences between the attention maps across the different heads.\n",
        "\n",
        "!!! YOUR ANSWER HERE !!!"
      ],
      "metadata": {
        "id": "PTyaP3JQyhd-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(ELEC/COMP 546) Problem 2b: PCA analysis on output feature patches."
      ],
      "metadata": {
        "id": "rAmy7NAfUBLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "# Your code for Problem 2b goes here"
      ],
      "metadata": {
        "id": "Q0JO3JGys3Yi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 2b: Comment on how the feature patches from similar objects (i.e. the dogs) are colored.\n",
        "\n",
        "!!! YOUR ANSWER HERE !!!"
      ],
      "metadata": {
        "id": "iE_iEtQAyx-8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 3: Using CLIP for Zero-Shot Classification\n"
      ],
      "metadata": {
        "id": "j97jpcKfoYbX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "id": "9-OE42rWoYrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import clip\n",
        "\n",
        "model, preprocess = clip.load(\"ViT-B/32\")\n",
        "model.cuda().eval()\n",
        "input_resolution = model.visual.input_resolution\n",
        "context_length = model.context_length\n",
        "vocab_size = model.vocab_size\n",
        "\n",
        "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
        "print(\"Input resolution:\", input_resolution)\n",
        "print(\"Context length:\", context_length)\n",
        "print(\"Vocab size:\", vocab_size)"
      ],
      "metadata": {
        "id": "9eggbF6cobRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "cifar = np.load('/content/drive/MyDrive/ELEC 477/CIFAR.npz') # Replace with your path to CIFAR.\n",
        "X,y,label_names = cifar['X'], cifar['y']*1.0, cifar['label_names']\n",
        "print(label_names)"
      ],
      "metadata": {
        "id": "pD456-llodLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 3a: Implement zero-shot classification with CLIP"
      ],
      "metadata": {
        "id": "NLUTgbsUzC9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "captions = None # Your code goes here.\n",
        "\n",
        "# Iterate over all test examples.\n",
        "for i in tqdm(range(50000, 60000)):\n",
        "  image = preprocess(Image.fromarray(np.uint8(X[i,...]))).unsqueeze(0).to('cuda')\n",
        "  text = clip.tokenize(captions).to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    logits_per_image, logits_per_text = model(image, text)\n",
        "    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
        "\n",
        "    # Your code goes here.\n",
        "\n",
        "# Your code goes here."
      ],
      "metadata": {
        "id": "4OglZ3bUoex3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 3b: Prompt engineering for zero-shot classification"
      ],
      "metadata": {
        "id": "jIATMHxQzkNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "engineered_captions = None # Your code goes here.\n",
        "\n",
        "# Iterate over all test examples.\n",
        "for i in tqdm(range(50000, 60000)):\n",
        "  image = preprocess(Image.fromarray(np.uint8(X[i,...]))).unsqueeze(0).to('cuda')\n",
        "  text = clip.tokenize(engineered_captions).to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    logits_per_image, logits_per_text = model(image, text)\n",
        "    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
        "\n",
        "    # Your code goes here.\n",
        "\n",
        "# Your code goes here."
      ],
      "metadata": {
        "id": "0CnMR_-CzXiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 4: StyleGAN\n"
      ],
      "metadata": {
        "id": "pY92owDyn_Zz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# setup correct PyTorch version\n",
        "!pip install -U torch==1.7.1+cu110 torchvision==0.8.2+cu110 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "import torch\n",
        "\n",
        "# Download the code\n",
        "!git clone https://github.com/NVlabs/stylegan2-ada-pytorch.git\n",
        "%cd stylegan2-ada-pytorch\n",
        "\n",
        "# install other dependencies\n",
        "!pip install ninja\n",
        "\n",
        "print('PyTorch version: {}'.format(torch.__version__) )\n",
        "!nvidia-smi -L\n",
        "print('GPU Identified at: {}'.format(torch.cuda.get_device_name()))"
      ],
      "metadata": {
        "id": "uKaMR3Y3oAgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the model\n",
        "import argparse\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import dnnlib\n",
        "import re\n",
        "import sys\n",
        "from io import BytesIO\n",
        "import IPython.display\n",
        "import numpy as np\n",
        "from math import ceil\n",
        "from PIL import Image, ImageDraw\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import legacy\n",
        "import cv2\n",
        "import torch\n",
        "from tqdm.autonotebook import tqdm\n",
        "\n",
        "device = torch.device('cuda')\n",
        "\n",
        "# Choose between these pretrained models\n",
        "# https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/afhqcat.pkl\n",
        "# https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/afhqdog.pkl\n",
        "# https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/afhqwild.pkl\n",
        "# https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/brecahad.pkl\n",
        "# https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/cifar10.pkl\n",
        "# https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl\n",
        "# https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metfaces.pkl\n",
        "\n",
        "network_pkl = \"https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl\"\n",
        "\n",
        "# If downloads fails, you can try downloading manually and uploading to the session directly\n",
        "# network_pkl = \"/content/ffhq.pkl\"\n",
        "\n",
        "print('Loading networks from \"%s\"...' % network_pkl)\n",
        "with dnnlib.util.open_url(network_pkl) as f:\n",
        "  G = legacy.load_network_pkl(f)['G_ema'].to(device) # type: ignore"
      ],
      "metadata": {
        "id": "99YLKSKPoCsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Useful utility functions...\n",
        "\n",
        "# Generates an image from a style vector.\n",
        "def generate_image_from_style(dlatent, noise_mode='none'):\n",
        "\n",
        "  if len(dlatent.shape) == 1:\n",
        "    dlatent = dlatent.unsqueeze(0)\n",
        "\n",
        "  row_images = G.synthesis(dlatent, noise_mode=noise_mode)\n",
        "  row_images = (row_images.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n",
        "  return row_images[0].cpu().numpy()\n",
        "\n",
        "# Converts a noise vector z to a style vector w.\n",
        "def convert_z_to_w(latent, truncation_psi=0.7, truncation_cutoff=9, class_idx=None):\n",
        "  label = torch.zeros([1, G.c_dim], device=device)\n",
        "  if G.c_dim != 0:\n",
        "    if class_idx is None:\n",
        "      RuntimeError('Must specify class label with class_idx when using a conditional network')\n",
        "    label[:, class_idx] = 1\n",
        "  else:\n",
        "    if class_idx is not None:\n",
        "      print(f'warning: class_idx={class_idx} ignored when running on an unconditional network')\n",
        "  return G.mapping(latent, label, truncation_psi=truncation_psi, truncation_cutoff=truncation_cutoff)"
      ],
      "metadata": {
        "id": "GzBGpxkxoHhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample code to generate images.\n",
        "np.random.seed(123) # You can change this random seed.\n",
        "\n",
        "# Generate a random noise (z) vector.\n",
        "z = torch.from_numpy(np.random.randn(1, G.z_dim)).to(device)\n",
        "\n",
        "# Convert z vector to w vector.\n",
        "w = convert_z_to_w(z, truncation_psi=0.7, truncation_cutoff=9)\n",
        "\n",
        "# Generate and show image.\n",
        "img = generate_image_from_style(w)\n",
        "plt.imshow(img)"
      ],
      "metadata": {
        "id": "44M3XpatoJeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LATENT SPACE FACE TRAVERSALS"
      ],
      "metadata": {
        "id": "0tiBXfevoLMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download\n",
        "!gdown \"1vekENF84yvVpKhMaChqTVEyttAckZ4PU\" -O \"../\""
      ],
      "metadata": {
        "id": "J8JXT0qloLkG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83fe5f84-53e8-418e-daaa-b38cd152809d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1vekENF84yvVpKhMaChqTVEyttAckZ4PU\n",
            "From (redirected): https://drive.google.com/uc?id=1vekENF84yvVpKhMaChqTVEyttAckZ4PU&confirm=t&uuid=8fd70fec-c83e-408b-a542-6bb49bd5e8aa\n",
            "To: /ffhq-Gender.weights\n",
            "100% 94.4M/94.4M [00:01<00:00, 87.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import models as tv\n",
        "cnn = tv.resnet50(pretrained=False, progress=True, num_classes = 1)\n",
        "cnn.eval()\n",
        "cnn.load_state_dict(torch.load('../ffhq-Gender.weights', map_location=lambda storage, loc: storage))\n",
        "\n",
        "# Returns whether face is perceptually female (True) or male (False) given\n",
        "# an input image of shape (H, W, 3).\n",
        "def face_is_female(img):\n",
        "  im = np.asarray(img)/255.0\n",
        "  im = cv2.resize(im, (256, 256))\n",
        "  im = np.expand_dims(np.transpose(im, (2,0,1)), 0)\n",
        "  im = torch.FloatTensor(im)\n",
        "  logits = cnn(im)[0, 0]\n",
        "  return (logits < 0.5).numpy()"
      ],
      "metadata": {
        "id": "6DieOCOHoNY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 4a: Interpolation between two faces and gender classification.\n"
      ],
      "metadata": {
        "id": "_gV4ncWQoQfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code goes here."
      ],
      "metadata": {
        "id": "wXBRDCRBoQvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 4a: What differences do you notice when interpolating in style space? Do the intermediate faces look realistic?\n",
        "\n",
        "!!! YOUR ANSWER HERE !!!"
      ],
      "metadata": {
        "id": "NxdUt-UA0C_m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 4b: Latent space traversals"
      ],
      "metadata": {
        "id": "5ityh8XRoURC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code goes here."
      ],
      "metadata": {
        "id": "ZQv6_a4loV4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 4b: Do you notice any facial attributes that seem to commonly change when moving between males and females? Why do you think that occurs?\n",
        "\n",
        "!!! YOUR ANSWER HERE !!!"
      ],
      "metadata": {
        "id": "iIfRF4Ce0QPt"
      }
    }
  ]
}